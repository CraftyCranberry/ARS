{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Qp4RaLfdR9bF"},"outputs":[],"source":["!apt-get install -y swig\n","!pip install \"gymnasium[all]\"\n","!pip install jax\n","!pip install numpy"]},{"cell_type":"markdown","source":["------------------------------------------------------------------------------\n","Below is the implimentation for solving Cartpole using QL (No Replay Buffer)\n","------------------------------------------------------------------------------"],"metadata":{"id":"lmMPr_Ki-66H"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERrMr0PLSUUH"},"outputs":[],"source":["#This code was adapted from Johnnyode8's GitHub repo: https://github.com/johnnycode8/gym_solutions\n","\n","import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import time\n","import argparse\n","import os\n","from distutils.util import strtobool\n","import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","import shutil\n","from google.colab import drive\n","\n","# Mount Google Drive for persistent storage\n","drive.mount('/content/drive')\n","\n","def run(is_training=True, render=False):\n","    # Hyperparameters\n","    learning_rate_a = 0.25  # Alpha: learning rate\n","    discount_factor_g = 0.99  # Gamma: discount factor\n","    epsilon = 1  # Exploration rate: 100% random actions initially\n","    epsilon_decay_rate = 0.0001  # Rate at which exploration decreases\n","    rng = np.random.default_rng()  # Random number generator\n","\n","    rewards_per_episode = []\n","    max_rewards = 0\n","    mean_rewards = 0\n","    episode = 0\n","\n","    # Logging and saving setup\n","    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    run_name = f\"CartPole_QL_LR_{learning_rate_a}_GAMMA_{discount_factor_g}_EPS_{epsilon}_DECAY_{epsilon_decay_rate}_{timestamp}\"\n","    local_log_path = f\"final_code/runs/{run_name}\"\n","    writer = SummaryWriter(local_log_path)\n","    source_folder = f\"final_code/runs/{run_name}\"\n","    destination_folder = '/content/drive/My Drive/RL/final_code/runs/'\n","\n","    # Environment setup\n","    env = gym.make('CartPole-v1', render_mode='human' if render else None)\n","\n","    # Discretizing the state space\n","    pos_space = np.linspace(-2.4, 2.4, 10)\n","    vel_space = np.linspace(-4, 4, 10)\n","    ang_space = np.linspace(-.2095, .2095, 10)\n","    ang_vel_space = np.linspace(-4, 4, 10)\n","\n","     # Q-table initialization\n","    if is_training:\n","        q = np.zeros((len(pos_space)+1, len(vel_space)+1, len(ang_space)+1, len(ang_vel_space)+1, env.action_space.n))\n","    else:\n","        # Load Q-table if not training\n","        file_path = '/content/drive/My Drive/RL/final_code/runs/q_tables/cartpole_QL.pkl'\n","        with open(file_path, 'rb') as f:\n","            q = pickle.load(f)\n","\n","    # Main loop\n","    while(True):\n","         # Reset and discretize initial state\n","        state = env.reset()[0]      # Starting position, starting velocity always 0\n","        state_p = np.digitize(state[0], pos_space)\n","        state_v = np.digitize(state[1], vel_space)\n","        state_a = np.digitize(state[2], ang_space)\n","        state_av = np.digitize(state[3], ang_vel_space)\n","\n","        terminated = False\n","        total_reward = 0\n","        episode_length = 0\n","\n","        while(not terminated and total_reward < 10000):\n","            episode_length += 1\n","\n","            if is_training and rng.random() < epsilon:\n","                # Choose random action  (0=go left, 1=go right)\n","                action = env.action_space.sample()\n","            else:\n","                action = np.argmax(q[state_p, state_v, state_a, state_av, :])\n","\n","            # Take action and observe new state and reward\n","            new_state,reward,terminated,_,_ = env.step(action)\n","            new_state_p = np.digitize(new_state[0], pos_space)\n","            new_state_v = np.digitize(new_state[1], vel_space)\n","            new_state_a = np.digitize(new_state[2], ang_space)\n","            new_state_av= np.digitize(new_state[3], ang_vel_space)\n","\n","            # Q-value update\n","            if is_training:\n","                q[state_p, state_v, state_a, state_av, action] = q[state_p, state_v, state_a, state_av, action] + learning_rate_a * (\n","                    reward + discount_factor_g*np.max(q[new_state_p, new_state_v, new_state_a, new_state_av,:]) - q[state_p, state_v, state_a, state_av, action])\n","\n","            state = new_state\n","            state_p = new_state_p\n","            state_v = new_state_v\n","            state_a = new_state_a\n","            state_av= new_state_av\n","\n","            total_reward += reward\n","\n","            # Monitoring training progress\n","            if not is_training and total_reward % 100 == 0:\n","                print(f'Episode: {episode}  Rewards: {total_reward}')\n","\n","        # Tensorboard logging\n","        if is_training and episode % 10 == 0:\n","            writer.add_scalar(\"charts/episodic_return\", total_reward, episode)\n","            writer.add_scalar(\"charts/epsilon\", epsilon, episode)\n","            writer.add_scalar(\"charts/episodic_length\", episode_length, episode)\n","            writer.add_scalar(\"charts/learning_rate\", learning_rate_a, episode)\n","            writer.add_scalar(\"charts/gamma\", discount_factor_g, episode)\n","            writer.add_scalar(\"charts/epsilon_decay_rate\", epsilon_decay_rate, episode)\n","\n","        epsilon = max(epsilon - epsilon_decay_rate, 0)  # Epsilon decay\n","\n","        # Save Q-table and logs\n","        if is_training and episode % 100 == 0:\n","            print(\"Saving Q-Table\")\n","            file_path = f\"/content/drive/My Drive/RL/final_code/runs/q_tables/{run_name}.pkl\"\n","            with open(file_path, 'wb') as f:\n","                pickle.dump(q, f)\n","            try:\n","                destination_path = f\"{destination_folder}/{source_folder.split('/')[-1]}\"\n","                shutil.copytree(source_folder, destination_path, dirs_exist_ok=True)\n","            except Exception as e:\n","                print(f\"Error in copying files: {e}\")\n","\n","        rewards_per_episode.append(total_reward)\n","\n","        # if len(rewards_per_episode) >= 100:\n","        #     mean_rewards = np.mean(rewards_per_episode[-100:])\n","        #     # Break if mean reward over the last 200 episodes is greater than 500\n","        #     if mean_rewards > 100:\n","        #         break\n","\n","        #Break after 15000 episodes\n","        if episode > 15000:\n","            break\n","\n","        if is_training and episode % 100 == 0:\n","            mean_rewards = np.mean(rewards_per_episode[-1000:])\n","            print(f'Episode: {episode} {total_reward}  Epsilon: {epsilon:.2f}  Mean Rewards: {mean_rewards:.1f}')\n","\n","        episode += 1\n","\n","    env.close()\n","\n","    # Plotting rewards\n","    mean_rewards = [np.mean(rewards_per_episode[max(0, t-100):(t+1)]) for t in range(episode)]\n","    plt.plot(mean_rewards)\n","    plt.savefig('cartpole_QL.png')\n","\n","if __name__ == '__main__':\n","\n","    run(is_training=True, render=False)\n"]},{"cell_type":"markdown","metadata":{"id":"_NvXukU1x3rS"},"source":["------------------------------------------------------------------------------\n","Below is the implimentation for solving Cartpole using QL with Replay Buffer implimented\n","------------------------------------------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVaeSDyZxtyY"},"outputs":[],"source":["#This code was adapted from Johnnyode8's GitHub repo: https://github.com/johnnycode8/gym_solutions\n","\n","import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import time\n","import argparse\n","import random\n","import os\n","from distutils.util import strtobool\n","import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","import shutil\n","from google.colab import drive\n","\n","# Mount Google Drive for persistent storage\n","drive.mount('/content/drive')\n","\n","\n","class ReplayBuffer:\n","    def __init__(self, size):\n","        self.buffer = []\n","        self.size = size\n","        self.next_idx = 0\n","\n","    def add(self, state, action, reward, next_state, done):\n","        data = (state, action, reward, next_state, done)\n","\n","        if self.next_idx >= len(self.buffer):\n","            self.buffer.append(data)\n","        else:\n","            self.buffer[self.next_idx] = data\n","\n","        self.next_idx = (self.next_idx + 1) % self.size\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.buffer, batch_size)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","\n","\n","def run(is_training=True, render=False):\n","    # Hyperparameters\n","    learning_rate_a = 0.25  # Alpha: learning rate\n","    discount_factor_g = 0.99  # Gamma: discount factor\n","    epsilon = 1  # Exploration rate: 100% random actions initially\n","    epsilon_decay_rate = 0.0001  # Rate at which exploration decreases\n","    batch_size = 10  # Define your batch size\n","    rng = np.random.default_rng()  # Random number generator\n","\n","    episode = 0\n","    rewards_per_episode = []\n","\n","    # Logging and saving setup\n","    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    run_name = f\"CartPole_QL_ReplayBuffer_LR_{learning_rate_a}_GAMMA_{discount_factor_g}_EPS_{epsilon}_DECAY_{epsilon_decay_rate}_{timestamp}_BatchSize=1\"\n","    local_log_path = f\"final_code/runs/{run_name}\"\n","    writer = SummaryWriter(local_log_path)\n","    source_folder = f\"final_code/runs/{run_name}\"\n","    destination_folder = '/content/drive/My Drive/RL/final_code/runs/'\n","\n","    # Environment setup\n","    env = gym.make('CartPole-v1', render_mode='human' if render else None)\n","\n","    # Replay buffer initialization\n","    buffer_size = 10000\n","    replay_buffer = ReplayBuffer(buffer_size)\n","\n","    # State space discretization\n","    pos_space = np.linspace(-2.4, 2.4, 10)\n","    vel_space = np.linspace(-4, 4, 10)\n","    ang_space = np.linspace(-.2095, .2095, 10)\n","    ang_vel_space = np.linspace(-4, 4, 10)\n","\n","    # Q-table initialization\n","    if is_training:\n","        q = np.zeros((len(pos_space)+1, len(vel_space)+1, len(ang_space)+1, len(ang_vel_space)+1, env.action_space.n))\n","    else:\n","        # Load Q-table if not training\n","        file_path = '/content/drive/My Drive/RL/final_code/runs/q_tables/cartpole_QL_ReplayBuffer.pkl'\n","        with open(file_path, 'rb') as f:\n","            q = pickle.load(f)\n","\n","\n","\n","    while(True):\n","\n","        state = env.reset()[0]      # Starting position, starting velocity always 0\n","        state_p = np.digitize(state[0], pos_space)\n","        state_v = np.digitize(state[1], vel_space)\n","        state_a = np.digitize(state[2], ang_space)\n","        state_av = np.digitize(state[3], ang_vel_space)\n","\n","        terminated = False\n","        total_reward = 0\n","        episode_length = 0\n","\n","        while not terminated and total_reward < 10000:\n","            episode_length += 1\n","\n","            if is_training and rng.random() < epsilon:\n","                action = env.action_space.sample()\n","            else:\n","                action = np.argmax(q[state_p, state_v, state_a, state_av, :])\n","\n","            # Take action and observe new state and reward\n","            new_state,reward,terminated,_,_ = env.step(action)\n","            new_state_p = np.digitize(new_state[0], pos_space)\n","            new_state_v = np.digitize(new_state[1], vel_space)\n","            new_state_a = np.digitize(new_state[2], ang_space)\n","            new_state_av= np.digitize(new_state[3], ang_vel_space)\n","\n","            replay_buffer.add(state, action, reward, new_state, terminated)\n","\n","\n","            if is_training and len(replay_buffer) >= batch_size:\n","                mini_batch = replay_buffer.sample(batch_size)\n","\n","                for state, action, reward, next_state, done in mini_batch:\n","                    # Manually digitize each component of the state and next_state\n","                    state_p = np.digitize(state[0], pos_space)\n","                    state_v = np.digitize(state[1], vel_space)\n","                    state_a = np.digitize(state[2], ang_space)\n","                    state_av = np.digitize(state[3], ang_vel_space)\n","\n","                    next_state_p = np.digitize(next_state[0], pos_space)\n","                    next_state_v = np.digitize(next_state[1], vel_space)\n","                    next_state_a = np.digitize(next_state[2], ang_space)\n","                    next_state_av = np.digitize(next_state[3], ang_vel_space)\n","\n","                    # Calculate the target value\n","                    if done:\n","                        target = reward\n","                    else:\n","                        target = reward + discount_factor_g * np.max(q[next_state_p, next_state_v, next_state_a, next_state_av, :])\n","\n","                    # Update Q-value\n","                    q[state_p, state_v, state_a, state_av, action] = (1 - learning_rate_a) * q[state_p, state_v, state_a, state_av, action] + learning_rate_a * target\n","\n","            state = new_state\n","            state_p = new_state_p\n","            state_v = new_state_v\n","            state_a = new_state_a\n","            state_av= new_state_av\n","\n","            total_reward+=reward\n","\n","\n","            if not is_training and total_reward%100==0:\n","                print(f'Episode: {episode}  Rewards: {total_reward}')\n","\n","        # Tensorboard logging\n","        if is_training and episode % 10 == 0:\n","            writer.add_scalar(\"charts/episodic_return\", total_reward, episode)\n","            writer.add_scalar(\"charts/epsilon\", epsilon, episode)\n","            writer.add_scalar(\"charts/episodic_length\", episode_length, episode)\n","            writer.add_scalar(\"charts/learning_rate\", learning_rate_a, episode)\n","            writer.add_scalar(\"charts/gamma\", discount_factor_g, episode)\n","            writer.add_scalar(\"charts/epsilon_decay_rate\", epsilon_decay_rate, episode)\n","\n","        epsilon = max(epsilon - epsilon_decay_rate, 0)\n","\n","        # Save Q-table and logs\n","        if is_training and episode % 100 == 0:\n","            print(\"Saving Q-Table\")\n","            file_path = f\"/content/drive/My Drive/RL/final_code/runs/q_tables/{run_name}.pkl\"\n","            with open(file_path, 'wb') as f:\n","                pickle.dump(q, f)\n","            try:\n","                destination_path = f\"{destination_folder}/{source_folder.split('/')[-1]}\"\n","                shutil.copytree(source_folder, destination_path, dirs_exist_ok=True)\n","            except Exception as e:\n","                print(f\"Error in copying files: {e}\")\n","\n","        rewards_per_episode.append(total_reward)\n","\n","        # if len(rewards_per_episode) >= 100:\n","        #     mean_rewards = np.mean(rewards_per_episode[-100:])\n","        #     # Break if mean reward over the last 200 episodes is greater than 500\n","        #     if mean_rewards > 100:\n","        #         break\n","\n","        #Break after 15000 episodes\n","        if episode > 15000:\n","            break\n","\n","\n","        if is_training and episode % 100 == 0:\n","            mean_rewards = np.mean(rewards_per_episode[-1000:])\n","            print(f'Episode: {episode} {total_reward}  Epsilon: {epsilon:.2f}  Mean Rewards: {mean_rewards:.1f}')\n","\n","        episode += 1\n","\n","\n","\n","    env.close()\n","\n","    # Plotting rewards\n","    mean_rewards = [np.mean(rewards_per_episode[max(0, t-100):(t+1)]) for t in range(episode)]\n","    plt.plot(mean_rewards)\n","    plt.savefig('cartpole_QL_ReplayBuffer.png')\n","\n","if __name__ == '__main__':\n","\n","    run(is_training=True, render=False)\n","\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["lmMPr_Ki-66H","_NvXukU1x3rS"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}