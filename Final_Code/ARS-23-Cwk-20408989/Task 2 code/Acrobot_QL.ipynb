{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Jh4RrRbAzNAy"},"outputs":[],"source":["!apt-get install -y swig\n","!pip install \"gymnasium[all]\"\n","!pip install jax\n","!pip install numpy"]},{"cell_type":"markdown","source":["------------------------------------------------------------------------------\n","Below is the implimentation for solving Acrobot using QL (No Replay Buffer)\n","------------------------------------------------------------------------------"],"metadata":{"id":"N17GiJZLQAxA"}},{"cell_type":"code","source":["#This code was adapted from Johnnyode8's GitHub repo: https://github.com/johnnycode8/gym_solutions\n","\n","import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","\n","import time\n","import argparse\n","import os\n","from distutils.util import strtobool\n","import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","import shutil\n","from google.colab import drive\n","\n","# Mount Google Drive for storage\n","drive.mount('/content/drive')\n","\n","def run(is_training=True, render=False):\n","    # Hyperparameters\n","    learning_rate_a = 0.25  # Alpha: learning rate\n","    discount_factor_g = 0.99  # Gamma: discount factor\n","    epsilon = 1  # Exploration rate: 100% random actions initially\n","    epsilon_decay_rate = 0.0001  # Rate at which exploration decreases\n","    rng = np.random.default_rng()  # Random number generator\n","\n","    rewards_per_episode = []\n","    max_rewards = 0\n","    mean_rewards = 0\n","    episode = 0\n","\n","    # Logging and saving setup\n","    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    run_name = f\"Acrobot_QL_LR_{learning_rate_a}_GAMMA_{discount_factor_g}_EPS_{epsilon}_DECAY_{epsilon_decay_rate}_{timestamp}\"\n","    local_log_path = f\"final_code/runs/{run_name}\"\n","    writer = SummaryWriter(local_log_path)\n","    source_folder = f\"final_code/runs/{run_name}\"\n","    destination_folder = '/content/drive/My Drive/RL/final_code/runs/'\n","\n","    # Environment setup\n","    env = gym.make('Acrobot-v1', render_mode='human' if render else None)\n","\n","    # Discretizing the state space\n","    cos_t1 = np.linspace(-1, 1, 10)\n","    sin_t1 = np.linspace(-1, 1, 10)\n","    cos_t2 = np.linspace(-1, 1, 10)\n","    sin_t2 = np.linspace(-1, 1, 10)\n","    ang_vel_t1 = np.linspace(-12.567, 12.567, 20)\n","    ang_vel_t2 = np.linspace(-28.274, 28.274, 40)\n","\n","    # Q-table initialization\n","    if(is_training):\n","        q = np.zeros((len(cos_t1)+1, len(sin_t1)+1, len(cos_t2)+1, len(sin_t2)+1, len(ang_vel_t1)+1, len(ang_vel_t2)+1, env.action_space.n))\n","    else:\n","        # Load Q-table if not training\n","        file_path = '/content/drive/My Drive/RL/final_code/runs/q_tables/acrobot.pkl'\n","        with open(file_path, 'rb') as f:\n","            q = pickle.load(f)\n","\n","    # Main loop\n","    while(True):\n","        # Reset and discretize initial state\n","        state = env.reset()[0]      # Starting position, starting velocity always 0\n","        state_cos_t1 = np.digitize(state[0], cos_t1)\n","        state_sin_t1 = np.digitize(state[1], sin_t1)\n","        state_cos_t2 = np.digitize(state[2], cos_t2)\n","        state_sin_t2 = np.digitize(state[3], sin_t2)\n","        state_av_t1 = np.digitize(state[4], ang_vel_t1)\n","        state_av_t2 = np.digitize(state[5], ang_vel_t2)\n","\n","        terminated = False\n","        total_reward = 0\n","        episode_length = 0\n","\n","        while(not terminated or episode_length < -200):\n","            episode_length += 1\n","\n","            if is_training and rng.random() < epsilon:\n","                # Choose random action  (0=apply -1 torque, 1=apply 0 torque, 2=apply 1 torque)\n","                action = env.action_space.sample()\n","            else:\n","                action = np.argmax(q[state_cos_t1, state_sin_t1, state_cos_t2, state_sin_t2, state_av_t1, state_av_t2, :])\n","\n","            # Take action and observe new state and reward\n","            new_state,reward,terminated,_,_ = env.step(action)\n","            new_state_cos_t1 = np.digitize(new_state[0], cos_t1)\n","            new_state_sin_t1 = np.digitize(new_state[1], sin_t1)\n","            new_state_cos_t2 = np.digitize(new_state[2], cos_t2)\n","            new_state_sin_t2 = np.digitize(new_state[3], sin_t2)\n","            new_state_av_t1 = np.digitize(new_state[4], ang_vel_t1)\n","            new_state_av_t2 = np.digitize(new_state[5], ang_vel_t2)\n","\n","            # Q-value update\n","            if is_training:\n","                q[state_cos_t1, state_sin_t1, state_cos_t2, state_sin_t2, state_av_t1, state_av_t2, action] = q[state_cos_t1, state_sin_t1, state_cos_t2, state_sin_t2, state_av_t1, state_av_t2, action] + learning_rate_a * (\n","                    reward + discount_factor_g*np.max(q[new_state_cos_t1, new_state_sin_t1, new_state_cos_t2, new_state_sin_t2, new_state_av_t1, new_state_av_t2,:]) - q[state_cos_t1, state_sin_t1, state_cos_t2, state_sin_t2, state_av_t1, state_av_t2, action])\n","\n","            state = new_state\n","            state_cos_t1 = new_state_cos_t1\n","            state_sin_t1 = new_state_sin_t1\n","            state_cos_t2 = new_state_cos_t2\n","            state_sin_t2 = new_state_sin_t2\n","            state_av_t1 = new_state_av_t1\n","            state_av_t2 = new_state_av_t2\n","\n","            total_reward += reward\n","\n","             # Monitoring training progress\n","            if not is_training and total_reward % 100 == 0:\n","                print(f'Episode: {episode}  Rewards: {total_reward}')\n","\n","        # Tensorboard logging\n","        if is_training and episode % 10 == 0:\n","            writer.add_scalar(\"charts/episodic_return\", total_reward, episode)\n","            writer.add_scalar(\"charts/epsilon\", epsilon, episode)\n","            writer.add_scalar(\"charts/episodic_length\", episode_length, episode)\n","            writer.add_scalar(\"charts/learning_rate\", learning_rate_a, episode)\n","            writer.add_scalar(\"charts/gamma\", discount_factor_g, episode)\n","            writer.add_scalar(\"charts/epsilon_decay_rate\", epsilon_decay_rate, episode)\n","\n","        epsilon = max(epsilon - epsilon_decay_rate, 0)   # Epsilon decay\n","\n","        # Save Q-table and logs\n","        if is_training and episode % 100 == 0:\n","            print(\"Saving Q-Table\")\n","            file_path = f\"/content/drive/My Drive/RL/final_code/runs/q_tables/{run_name}.pkl\"\n","            with open(file_path, 'wb') as f:\n","                pickle.dump(q, f)\n","            try:\n","                destination_path = f\"{destination_folder}/{source_folder.split('/')[-1]}\"\n","                shutil.copytree(source_folder, destination_path, dirs_exist_ok=True)\n","            except Exception as e:\n","                print(f\"Error in copying files: {e}\")\n","\n","        rewards_per_episode.append(total_reward)\n","\n","        # if len(rewards_per_episode) >= 100:\n","        #     mean_rewards = np.mean(rewards_per_episode[-100:])\n","        #     # Break if mean reward over the last 200 episodes is greater than 500\n","        #     if mean_rewards > 100:\n","        #         break\n","\n","         #Break after 15000 episodes\n","        if episode > 15000:\n","            break\n","\n","        if is_training and episode % 100 == 0:\n","            mean_rewards = np.mean(rewards_per_episode[-1000:])\n","            print(f'Episode: {episode} {total_reward}  Epsilon: {epsilon:.2f}  Mean Rewards: {mean_rewards:.1f}')\n","\n","        episode += 1\n","\n","    env.close()\n","\n","    # Plotting rewards\n","    mean_rewards = [np.mean(rewards_per_episode[max(0, t-100):(t+1)]) for t in range(episode)]\n","    plt.plot(mean_rewards)\n","    plt.savefig(f'acrobot_QL.png')\n","\n","if __name__ == '__main__':\n","\n","    run(is_training=True, render=False)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOILseteP1Do","outputId":"7e1f5091-b47a-45b0-b1cc-b5fc31362523"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Saving Q-Table\n","Episode: 0 -4720.0  Epsilon: 1.00  Mean Rewards: -4720.0\n","Saving Q-Table\n","Episode: 100 -3361.0  Epsilon: 0.99  Mean Rewards: -2126.4\n","Saving Q-Table\n","Episode: 200 -1251.0  Epsilon: 0.98  Mean Rewards: -1972.6\n","Saving Q-Table\n","Episode: 300 -3527.0  Epsilon: 0.97  Mean Rewards: -1990.8\n","Saving Q-Table\n","Episode: 400 -481.0  Epsilon: 0.96  Mean Rewards: -1935.8\n","Saving Q-Table\n","Episode: 500 -621.0  Epsilon: 0.95  Mean Rewards: -1831.4\n","Saving Q-Table\n","Episode: 600 -1207.0  Epsilon: 0.94  Mean Rewards: -1735.1\n","Saving Q-Table\n","Episode: 700 -1128.0  Epsilon: 0.93  Mean Rewards: -1676.0\n","Saving Q-Table\n","Episode: 800 -1667.0  Epsilon: 0.92  Mean Rewards: -1618.7\n","Saving Q-Table\n","Episode: 900 -2899.0  Epsilon: 0.91  Mean Rewards: -1562.1\n","Saving Q-Table\n","Episode: 1000 -514.0  Epsilon: 0.90  Mean Rewards: -1499.9\n","Saving Q-Table\n","Episode: 1100 -1174.0  Epsilon: 0.89  Mean Rewards: -1384.0\n","Saving Q-Table\n","Episode: 1200 -1065.0  Epsilon: 0.88  Mean Rewards: -1299.6\n","Saving Q-Table\n","Episode: 1300 -840.0  Epsilon: 0.87  Mean Rewards: -1184.3\n","Saving Q-Table\n","Episode: 1400 -500.0  Epsilon: 0.86  Mean Rewards: -1095.0\n","Saving Q-Table\n","Episode: 1500 -671.0  Epsilon: 0.85  Mean Rewards: -1035.7\n","Saving Q-Table\n","Episode: 1600 -435.0  Epsilon: 0.84  Mean Rewards: -988.4\n","Saving Q-Table\n","Episode: 1700 -602.0  Epsilon: 0.83  Mean Rewards: -934.2\n","Saving Q-Table\n","Episode: 1800 -429.0  Epsilon: 0.82  Mean Rewards: -881.6\n","Saving Q-Table\n","Episode: 1900 -507.0  Epsilon: 0.81  Mean Rewards: -839.3\n","Saving Q-Table\n","Episode: 2000 -521.0  Epsilon: 0.80  Mean Rewards: -807.8\n"]}]},{"cell_type":"markdown","metadata":{"id":"UsTJ9jj41Tum"},"source":["------------------------------------------------------------------------------\n","Below is the implimentation for solving Acrobot using QL with Replay Buffer implimented\n","------------------------------------------------------------------------------"]},{"cell_type":"code","source":["#This code was adapted from Johnnyode8's GitHub repo: https://github.com/johnnycode8/gym_solutions\n","\n","import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import random\n","import time\n","import argparse\n","import os\n","from distutils.util import strtobool\n","import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","import shutil\n","from google.colab import drive\n","\n","# Mount Google Drive for storage\n","drive.mount('/content/drive')\n","\n","class ReplayBuffer:\n","    def __init__(self, size):\n","        self.buffer = []\n","        self.size = size\n","        self.next_idx = 0\n","\n","    def add(self, state, action, reward, next_state, done):\n","        data = (state, action, reward, next_state, done)\n","\n","        if self.next_idx >= len(self.buffer):\n","            self.buffer.append(data)\n","        else:\n","            self.buffer[self.next_idx] = data\n","\n","        self.next_idx = (self.next_idx + 1) % self.size\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.buffer, batch_size)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","\n","def run(is_training=True, render=False):\n","    # Hyperparameters\n","    learning_rate_a = 0.25  # Alpha: learning rate\n","    discount_factor_g = 0.99  # Gamma: discount factor\n","    epsilon = 1  # Exploration rate: 100% random actions initially\n","    epsilon_decay_rate = 0.0001  # Rate at which exploration decreases\n","    batch_size = 10  # Define your batch size\n","    rng = np.random.default_rng()  # Random number generator\n","\n","    episode = 0\n","    rewards_per_episode = []\n","\n","    # Logging and saving setup\n","    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    run_name = f\"Acrobot_QL_ReplayBuffer_LR_{learning_rate_a}_GAMMA_{discount_factor_g}_EPS_{epsilon}_DECAY_{epsilon_decay_rate}_{timestamp}\"\n","    local_log_path = f\"final_code/runs/{run_name}\"\n","    writer = SummaryWriter(local_log_path)\n","    source_folder = f\"final_code/runs/{run_name}\"\n","    destination_folder = '/content/drive/My Drive/RL/final_code/runs/'\n","\n","    # Environment setup\n","    env = gym.make('Acrobot-v1', render_mode='human' if render else None)\n","\n","\n","    # Replay buffer initialization\n","    buffer_size = 10000  # The size of the buffer\n","    replay_buffer = ReplayBuffer(buffer_size)\n","\n","    # State space discretization\n","    cos_t1 = np.linspace(-1, 1, 10)\n","    sin_t1 = np.linspace(-1, 1, 10)\n","    cos_t2 = np.linspace(-1, 1, 10)\n","    sin_t2 = np.linspace(-1, 1, 10)\n","    ang_vel_t1 = np.linspace(-12.567, 12.567, 20)\n","    ang_vel_t2 = np.linspace(-28.274, 28.274, 40)\n","\n","    # Q-table initialization\n","    if(is_training):\n","        q = np.zeros((len(cos_t1)+1, len(sin_t1)+1, len(cos_t2)+1, len(sin_t2)+1, len(ang_vel_t1)+1, len(ang_vel_t2)+1, env.action_space.n))\n","    else:\n","        # Load Q-table if not training\n","        file_path = '/content/drive/My Drive/RL/final_code/runs/q_tables/acrobot_QL_ReplayBuffer.pkl'\n","        with open(file_path, 'rb') as f:\n","            q = pickle.load(f)\n","\n","\n","\n","    while(True):\n","\n","        state = env.reset()[0]      # Starting position, starting velocity always 0\n","        state_cos_t1 = np.digitize(state[0], cos_t1)\n","        state_sin_t1 = np.digitize(state[1], sin_t1)\n","        state_cos_t2 = np.digitize(state[2], cos_t2)\n","        state_sin_t2 = np.digitize(state[3], sin_t2)\n","        state_av_t1 = np.digitize(state[4], ang_vel_t1)\n","        state_av_t2 = np.digitize(state[5], ang_vel_t2)\n","\n","        terminated = False\n","        total_reward = 0\n","        episode_length = 0\n","\n","        while(not terminated or episode_length < -200):\n","            episode_length += 1\n","\n","            if is_training and rng.random() < epsilon:\n","                # Choose random action  (0=apply -1 torque, 1=apply 0 torque, 2=apply 1 torque)\n","                action = env.action_space.sample()\n","            else:\n","                action = np.argmax(q[state_cos_t1, state_sin_t1, state_cos_t2, state_sin_t2, state_av_t1, state_av_t2, :])\n","\n","            # Take action and observe new state and reward\n","            new_state,reward,terminated,_,_ = env.step(action)\n","            new_state_cos_t1 = np.digitize(new_state[0], cos_t1)\n","            new_state_sin_t1 = np.digitize(new_state[1], sin_t1)\n","            new_state_cos_t2 = np.digitize(new_state[2], cos_t2)\n","            new_state_sin_t2 = np.digitize(new_state[3], sin_t2)\n","            new_state_av_t1 = np.digitize(new_state[4], ang_vel_t1)\n","            new_state_av_t2 = np.digitize(new_state[5], ang_vel_t2)\n","\n","            replay_buffer.add(state, action, reward, new_state, terminated)\n","\n","\n","            if is_training and len(replay_buffer) >= batch_size:\n","                mini_batch = replay_buffer.sample(batch_size)\n","\n","                for state, action, reward, next_state, done in mini_batch:\n","                    # Manually digitize each component of the state and next_state\n","                    state_cos_t1 = np.digitize(state[0], cos_t1)\n","                    state_sin_t1 = np.digitize(state[1], sin_t1)\n","                    state_cos_t2 = np.digitize(state[2], cos_t2)\n","                    state_sin_t2 = np.digitize(state[3], sin_t2)\n","                    state_av_t1 = np.digitize(state[4], ang_vel_t1)\n","                    state_av_t2 = np.digitize(state[5], ang_vel_t2)\n","\n","                    next_state_cos_t1 = np.digitize(new_state[0], cos_t1)\n","                    next_state_sin_t1 = np.digitize(new_state[1], sin_t1)\n","                    next_state_cos_t2 = np.digitize(new_state[2], cos_t2)\n","                    next_state_sin_t2 = np.digitize(new_state[3], sin_t2)\n","                    next_state_av_t1 = np.digitize(new_state[4], ang_vel_t1)\n","                    next_state_av_t2 = np.digitize(new_state[5], ang_vel_t2)\n","\n","                    # Calculate the target value\n","                    if done:\n","                        target = reward\n","                    else:\n","                        target = reward + discount_factor_g * np.max(q[next_state_cos_t1, next_state_sin_t1, next_state_cos_t2, next_state_sin_t2, next_state_av_t1, next_state_av_t2, :])\n","\n","                    # Update Q-value\n","                    q[state_cos_t1, state_sin_t1, state_cos_t2, state_sin_t2, state_av_t1, state_av_t1, action] = (1 - learning_rate_a) * q[state_cos_t1, state_sin_t1, state_cos_t2, state_sin_t2, state_av_t1, state_av_t1, action] + learning_rate_a * target\n","\n","            state = new_state\n","            state_cos_t1 = new_state_cos_t1\n","            state_sin_t1 = new_state_sin_t1\n","            state_cos_t2 = new_state_cos_t2\n","            state_sin_t2 = new_state_sin_t2\n","            state_av_t1 = new_state_av_t1\n","            state_av_t2 = new_state_av_t2\n","\n","            total_reward += reward\n","\n","            if not is_training and total_reward%100==0:\n","                print(f'Episode: {episode}  Rewards: {reward}')\n","\n","        # Tensorboard logging\n","        if is_training and episode % 10 == 0:\n","            writer.add_scalar(\"charts/episodic_return\", total_reward, episode)\n","            writer.add_scalar(\"charts/epsilon\", epsilon, episode)\n","            writer.add_scalar(\"charts/episodic_length\", episode_length, episode)\n","            writer.add_scalar(\"charts/learning_rate\", learning_rate_a, episode)\n","            writer.add_scalar(\"charts/gamma\", discount_factor_g, episode)\n","            writer.add_scalar(\"charts/epsilon_decay_rate\", epsilon_decay_rate, episode)\n","\n","        epsilon = max(epsilon - epsilon_decay_rate, 0)\n","\n","        # Save Q-table and logs\n","        if is_training and episode % 100 == 0:\n","            print(\"Saving Q-Table\")\n","            file_path = f\"/content/drive/My Drive/RL/final_code/runs/q_tables/{run_name}.pkl\"\n","            with open(file_path, 'wb') as f:\n","                pickle.dump(q, f)\n","            try:\n","                destination_path = f\"{destination_folder}/{source_folder.split('/')[-1]}\"\n","                shutil.copytree(source_folder, destination_path, dirs_exist_ok=True)\n","            except Exception as e:\n","                print(f\"Error in copying files: {e}\")\n","\n","        rewards_per_episode.append(total_reward)\n","\n","        # if len(rewards_per_episode) >= 100:\n","        #     mean_rewards = np.mean(rewards_per_episode[-100:])\n","        #     # Break if mean reward over the last 200 episodes is greater than 500\n","        #     if mean_rewards > 100:\n","        #         break\n","\n","        #Break after 15000 episodes\n","        if episode > 15000:\n","            break\n","\n","        if is_training and episode % 100 == 0:\n","            mean_rewards = np.mean(rewards_per_episode[-1000:])\n","            print(f'Episode: {episode} {total_reward}  Epsilon: {epsilon:.2f}  Mean Rewards: {mean_rewards:.1f}')\n","\n","        episode += 1\n","\n","    env.close()\n","\n","    # Plotting rewards\n","    mean_rewards = [np.mean(rewards_per_episode[max(0, t-100):(t+1)]) for t in range(episode)]\n","    plt.plot(mean_rewards)\n","    plt.savefig(f'acrobot_QL_ReplayBuffer.png')\n","\n","if __name__ == '__main__':\n","\n","    run(is_training=True, render=False)\n","\n"],"metadata":{"id":"s6dzDmv7UEz9"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["N17GiJZLQAxA","UsTJ9jj41Tum"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}